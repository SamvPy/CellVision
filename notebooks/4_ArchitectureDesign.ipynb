{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25eeac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lance\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bd354",
   "metadata": {},
   "source": [
    "# Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66534a7",
   "metadata": {},
   "source": [
    "I have choosen two completely different model architectures to test my POC on. These two are entirely different, namely, a variational autoencoder and a contrastive learning-based framework (simCLR).\n",
    "\n",
    "There are some points to make for choosing a generative framework like a VAE:\n",
    "\n",
    "## Variational Autoencoder\n",
    "\n",
    "### Pro's\n",
    "- It allows me to make sliders on latent factors for **explainability** purposes\n",
    "- The latent space is inherently smooth\n",
    "- I could enforce certain latent factors to encode a specific characteristic by creating subbranches from the latent variables towards subgoals (e.g. size, circularity, class [conditional VAE]).\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "- Reconstructions will be blurred (averaged) and are not the best in quality\n",
    "- Learning might prove tricky \n",
    "\n",
    "## Contrastive learning\n",
    "\n",
    "- Might prove easier to train\n",
    "- Embeddings are much more powerful\n",
    "\n",
    "## A combination\n",
    "\n",
    "In the ideal case, I would combine the strenght of both. A simCLR encoder for embedding generation and a variational decoder on top for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4aea3b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = '/home/sam/SCI/cellenONE_project/datasets'\n",
    "\n",
    "lds = lance.dataset(\n",
    "    os.path.join(\n",
    "        root_data,\n",
    "        'test_Backup_SCI.lance'\n",
    "    )\n",
    ")\n",
    "\n",
    "dtps = lds.to_table(\n",
    "    columns=['cell_diff_crop'],\n",
    "    batch_size=24\n",
    ").to_batches()\n",
    "\n",
    "# dtps['cell_diff_crop'] = dtps['cell_diff_crop'].apply(lambda x: np.array(\n",
    "#     x.tolist(),\n",
    "#     dtype=float\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d9aed656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CellVision.data.preprocessing.augmentations import SimpleAugmentor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "53787691",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dtps:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5765e181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'targets': torch.Size([24, 64, 64]),\n",
       " 'aug_1': torch.Size([24, 64, 64]),\n",
       " 'aug_2': torch.Size([24, 64, 64])}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageToTensor = T.Compose(\n",
    "    [T.ToTensor()]\n",
    ")\n",
    "\n",
    "cells = [\n",
    "    Image.fromarray(\n",
    "        np.array(img, dtype=float)\n",
    "    ).convert('L') for img in batch[\"cell_diff_crop\"].tolist()]\n",
    "\n",
    "aug_1 = torch.cat([SimpleAugmentor(x) for x in cells])\n",
    "aug_2 = torch.cat([SimpleAugmentor(x) for x in cells])\n",
    "targets = torch.cat([ImageToTensor(x) for x in cells])\n",
    "\n",
    "\n",
    "{\n",
    "    'targets': targets.shape, # add channel dimension\n",
    "    'aug_1': aug_1.shape,\n",
    "    'aug_2': aug_2.shape\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da397cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "ImageToTensor(cells[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57d576f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 64, 64])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(augs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51722a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(cell).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cde3b",
   "metadata": {},
   "source": [
    "## 0. Helper blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6d999be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Three backbones (PyTorch) for small grayscale images (25-45 px).\n",
    "- BetaVAE: encoder -> latent (mu, logvar) -> decoder\n",
    "- ContrastiveModel: encoder (GAP) -> projection head (for SimCLR/BYOL/SimSiam)\n",
    "- Hybrid: contrastive encoder -> bottleneck VAE on top (decode from z)\n",
    "\n",
    "Design goals / notes:\n",
    "- Use Global Average Pooling (GAP) to reduce positional encoding\n",
    "- Small capacity to match tiny images (~40x40)\n",
    "- Inputs: 1-channel grayscale; adjust `in_ch` if needed\n",
    "- Recommended use: resize/pad images to a stable size (e.g., 40x40 or pad to 56x56 and random crop)\n",
    "\n",
    "\"\"\"\n",
    "# ---------------------------\n",
    "# Utility blocks\n",
    "# ---------------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel=3, stride=1, padding=1, use_bn=True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_c, out_c, kernel, stride, padding, bias=not use_bn)]\n",
    "        if use_bn:\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Encoder (shared ideas)\n",
    "# ---------------------------\n",
    "class SmallEncoderGAP(nn.Module):\n",
    "    \"\"\"Small convolutional encoder that ends with Global Average Pooling.\n",
    "    Output is a vector (features) suitable for projection head or VAE bottleneck.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, base_filters=16, out_feat=32):\n",
    "        super().__init__()\n",
    "        # design: conv -> conv(strided) -> conv(strided) -> GAP -> fc\n",
    "        self.conv1 = ConvBlock(in_ch, base_filters, stride=1)\n",
    "        self.conv2 = ConvBlock(base_filters, base_filters * 2, stride=2)  # downsample\n",
    "        self.conv3 = ConvBlock(base_filters * 2, base_filters * 4, stride=2)  # downsample\n",
    "        self.conv4 = ConvBlock(base_filters * 4, base_filters * 8, stride=2)  # downsample\n",
    "        self.gap = GlobalAvgPool2d()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(base_filters * 8, out_feat),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W) expected small H,W\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.gap(x)  # (B, channels)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Decoder for VAE (simple upsampling)\n",
    "# ---------------------------\n",
    "class SmallDecoder(nn.Module):\n",
    "    \"\"\"Simple decoder that maps latent z to image of specified size.\n",
    "    Uses linear -> reshape -> convtranspose / upsample conv.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=16, out_ch=1, base_filters=16, out_size=64):\n",
    "        super().__init__()\n",
    "        # compute a small spatial size to reshape into\n",
    "        # we'll reshape into (base_filters*4, h', w') where h'*w' approx = out_size//4 square\n",
    "        self.out_size = out_size\n",
    "        # choose a fixed small spatial grid: e.g., 5x5 if out_size ~40 (5*8=40 with upsample)\n",
    "        # We'll use a simple decoder that upsamples twice.\n",
    "        hidden_ch = base_filters * 8\n",
    "        self.fc = nn.Linear(z_dim, hidden_ch * 8 * 8)\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ConvBlock(hidden_ch, base_filters * 4),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ConvBlock(base_filters * 4, base_filters * 2),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ConvBlock(base_filters * 2, base_filters),\n",
    "            nn.Conv2d(base_filters, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: (B, z_dim)\n",
    "        B = z.size(0)\n",
    "        x = self.fc(z)\n",
    "        x = x.view(B, -1, 8, 8)  # (B, hidden_ch, 5, 5)\n",
    "        x = self.up(x)\n",
    "        # now x is larger than 40x40 depending on upsample specifics; center-crop / interpolate to out_size\n",
    "        # x = F.interpolate(x, size=(self.out_size, self.out_size), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Contrastive backbone + projection head\n",
    "# ---------------------------\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"Simple MLP projection head used in SimCLR/BYOL etc.\n",
    "    maps encoder features to projection space for contrastive loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=128, hidden_dim=128, out_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_b = dtps['cell_diff_crop'][:5].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7048c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 64, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor(dtps['cell_diff_crop'][:5].tolist()).unsqueeze(dim=1).float()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60437218",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SmallEncoderGAP(\n",
    "    in_ch=1,\n",
    "    base_filters=16,\n",
    "    out_feat=16\n",
    ")\n",
    "decoder = SmallDecoder(\n",
    "    z_dim=16,\n",
    "    out_ch=1,\n",
    "    base_filters=16,\n",
    "    out_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d19cf0",
   "metadata": {},
   "source": [
    "- torch.Size([5, 1, 64, 64])\n",
    "- Conv1: torch.Size([5, 16, 64, 64])\n",
    "- Conv2: torch.Size([5, 32, 32, 32])\n",
    "- Conv3: torch.Size([5, 64, 16, 16])\n",
    "- Conv4: torch.Size([5, 128, 8, 8])\n",
    "- GAP: torch.Size([5, 128])\n",
    "- FC: torch.Size([5, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "951f03bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 64, 64])\n",
      "torch.Size([5, 16])\n",
      "torch.Size([5, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "\n",
    "encoded = encoder(test)\n",
    "print(encoded.shape)\n",
    "\n",
    "decoded = decoder(encoded)\n",
    "print(decoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b88b3",
   "metadata": {},
   "source": [
    "# 1. beta Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE(nn.Module):\n",
    "    \"\"\"Small beta-VAE using SmallEncoderGAP as encoder trunk.\n",
    "    Encoder returns mu, logvar. Decoder reconstructs canonical centered image.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, z_dim=8, base_filters=16, hidden_feat=16, out_size=64, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.encoder_trunk = SmallEncoderGAP(in_ch=in_ch, base_filters=base_filters, out_feat=hidden_feat)\n",
    "        self.fc_mu = nn.Linear(hidden_feat, z_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_feat, z_dim)\n",
    "        self.decoder = SmallDecoder(z_dim=z_dim, out_ch=in_ch, base_filters=base_filters, out_size=out_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder_trunk(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65345d4",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "106d2efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 64, 64])\n",
      "torch.Size([5, 1, 64, 64]) torch.Size([5, 8]) torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "vae = BetaVAE(\n",
    "    in_ch=1,\n",
    "    z_dim=8,\n",
    "    base_filters=16,\n",
    "    hidden_feat=16,\n",
    "    out_size=64,\n",
    "    beta=1\n",
    ")\n",
    "\n",
    "print(test.shape)\n",
    "reconstructed, mu, logvar = vae(test)\n",
    "print(reconstructed.shape, mu.shape, logvar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399fb14",
   "metadata": {},
   "source": [
    "# 2. Contrastive Learning Model (simCLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1507605",
   "metadata": {},
   "source": [
    "Fix the down and upsampling towards my 68x68 dimensions so as to also remove the interpolate.\n",
    "\n",
    "Add the following as hyperparameters for VAE:\n",
    "- channels (filters)\n",
    "- layers\n",
    "- latent space dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b37a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    \"\"\"Encoder trunk + projection head. Use with SimCLR/BYOL training loops.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, base_filters=16, feat_dim=128, proj_hidden=128, proj_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder_trunk = SmallEncoderGAP(in_ch=in_ch, base_filters=base_filters, out_feat=feat_dim)\n",
    "        self.proj = ProjectionHead(in_dim=feat_dim, hidden_dim=proj_hidden, out_dim=proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.encoder_trunk(x)\n",
    "        z = self.proj(feats)\n",
    "        # optionally normalize for NT-Xent\n",
    "        z = F.normalize(z, dim=1)\n",
    "        return feats, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd6a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "816a1a22",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e2815",
   "metadata": {},
   "source": [
    "# 3. The Hybrid (skip for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Hybrid: contrastive encoder + VAE on top\n",
    "# ---------------------------\n",
    "class ContrastiveVAE(nn.Module):\n",
    "    \"\"\"Hybrid model: encoder trunk (contrastive) -> bottleneck (mu/logvar) -> decoder\n",
    "    Idea: train encoder with contrastive loss; then attach VAE head to same features and train decoder (or fine-tune jointly).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, base_filters=16, feat_dim=128, z_dim=64, decoder_base=16, out_size=40, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        # encoder trunk identical to contrastive backbone\n",
    "        self.encoder_trunk = SmallEncoderGAP(in_ch=in_ch, base_filters=base_filters, out_feat=feat_dim)\n",
    "        # VAE projection\n",
    "        self.fc_mu = nn.Linear(feat_dim, z_dim)\n",
    "        self.fc_logvar = nn.Linear(feat_dim, z_dim)\n",
    "        # decoder decodes from z\n",
    "        self.decoder = SmallDecoder(z_dim=z_dim, out_ch=in_ch, base_filters=decoder_base, out_size=out_size)\n",
    "\n",
    "    def encode_to_feats(self, x):\n",
    "        return self.encoder_trunk(x)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encode_to_feats(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.encode_to_feats(x)\n",
    "        mu = self.fc_mu(feats)\n",
    "        logvar = self.fc_logvar(feats)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return feats, recon, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b09c5d",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68471e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellenone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
